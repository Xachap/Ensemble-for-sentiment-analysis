{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Desarrollo del experimento.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw0ov22IeEob",
        "colab_type": "text"
      },
      "source": [
        "Importamos las dependencias necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5JQo-p5eAAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d89dff46-73a3-4329-e96c-efc41b6f9458"
      },
      "source": [
        "# Para cargar datasets desde Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Para preprocesar los datasets\n",
        "import numpy as np\n",
        "from numpy import asarray, zeros, random\n",
        "import pandas as pd\n",
        "\n",
        "# Para dividir el conjunto de datos en train y test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import resample, shuffle\n",
        "\n",
        "# Para aprender word embeddings\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Para construir el modelo\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, Flatten, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import load_model\n",
        "from keras.utils import np_utils\n",
        "from keras import optimizers\n",
        "\n",
        "# Para representaciones gráficas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Otras operaciones\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3NaBuOJeYBd",
        "colab_type": "text"
      },
      "source": [
        "Cargamos desde Google Drive el dataset preprocesado con OpenRefine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9GhhJ2PeW9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "77a028f4-f2b8-4a27-d05d-34c9af000b70"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data = pd.read_csv('/content/gdrive/My Drive/Hotel_BINARY.csv')\n",
        "data.head(3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Reviewer_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I am so angry that i made this post available...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No Negative No real complaints the hotel was g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rooms are nice but for elderly a bit difficul...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Reviewer_Score\n",
              "0   I am so angry that i made this post available...               0\n",
              "1  No Negative No real complaints the hotel was g...               1\n",
              "2   Rooms are nice but for elderly a bit difficul...               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAL2J-ReyqfT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWccgz-7yqH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR1_Ags4e5T_",
        "colab_type": "text"
      },
      "source": [
        "Dado que sólo vamos a usar una pequeña parte de registros para el entrenamiento (entre 8000 y 15000) por modelo, dividimos el dataset en training y test. Este último tendrá 3000 registros para agilizar el proceso de testing posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBNXsUEvfXA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# División en training y test\n",
        "train, test = train_test_split(data, test_size=0.02, random_state = seed)\n",
        "\n",
        "# Usaremos sólo los 3000 primeros registros de la parte de test\n",
        "test = test.iloc[:3000]\n",
        "\n",
        "# Longitud máxima para las reviews\n",
        "max_length = 600"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVxLLtmuf0YZ",
        "colab_type": "text"
      },
      "source": [
        "Un elemento fundamental es el Tokenizer, que nos permite convertir cada palabra en un número o conjunto de ellos, para que puedan servir de entrada a la red neuronal que se creará más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36O3nxEVgNaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "max_features = 1000\n",
        "#\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "#\n",
        "tokenizer.fit_on_texts(data['Review'].values)\n",
        "#\n",
        "vocab = len(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzTL0vbxquny",
        "colab_type": "text"
      },
      "source": [
        "En nuestro ensemble, dos de los modelos estarán creados con la ayuda de los vectores de palabras de GloVe, que han sido preentrenados y se encuentran disponibles para su descarga desde su [página oficial](https://nlp.stanford.edu/projects/glove/). Por motivos de recursos, usaremos su versión más pequeña, disponible para su descarga [aquí](http://nlp.stanford.edu/data/glove.6B.zip). Esta versión ha sido entrenada a partir de textos extraidos de Wikipedia y de varias plataformas de noticias internacionales, como el New York Times o el Washington Post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYxCdej1quyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "532dd8f3-9386-4146-f681-72170d36f72c"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/gdrive/My Drive/glove.6B.50d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab, 50))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kokf4eTjobFB",
        "colab_type": "text"
      },
      "source": [
        "Ya que vamos a crear varios modelos de redes neuronales, es necesario guardarlas en una lista, así como sus hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-FwXlTWoaNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = []\n",
        "n_registers = []\n",
        "batch_sizes = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6__p-hMhWgf",
        "colab_type": "text"
      },
      "source": [
        "La primera red neuronal tiene una estructura simple, formada unicamente por una capa Dropout y una capa LSTM. Como aclaración, la capa de Dropout es un elemento que ayuda a mitigar el sobreajuste, desactivando aleatoriamente un porcentaje definido de enlaces durante cada paso de entrenamiento. Definimos su estructura y la compilamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDPooplyjc8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "5bad9bd7-f878-43d4-d46e-4b04c8824af8"
      },
      "source": [
        "# Hiperparámetros de la red E0\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "# Inicialización\n",
        "model0 = Sequential()\n",
        "# Capa de entrada\n",
        "model0.add(Embedding(max_features, embed_dim,input_length = max_length))\n",
        "# Capa Dropout\n",
        "model0.add(SpatialDropout1D(0.4))\n",
        "# Capa LSTM\n",
        "model0.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "# Capa con dos salidas\n",
        "#   Salida 0: emociones negativas\n",
        "#   Salida 1: emociones positivas\n",
        "model0.add(Dense(2,activation='softmax'))\n",
        "# Compilamos el modelo\n",
        "model0.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# Mostramos su información\n",
        "print(model0.summary())\n",
        "\n",
        "# Guardamos el modelo en la lista de modelos, al igual que el tamaño de batch y el número de registros\n",
        "models.append(model0)\n",
        "batch_sizes.append(128)\n",
        "n_registers.append(8000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0701 13:50:40.821790 140041750001536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0701 13:50:40.887632 140041750001536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0701 13:50:40.896921 140041750001536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0701 13:50:40.928382 140041750001536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0701 13:50:40.940137 140041750001536 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0701 13:50:41.477819 140041750001536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0701 13:50:41.507675 140041750001536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 600, 128)          128000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 600, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 394       \n",
            "=================================================================\n",
            "Total params: 383,194\n",
            "Trainable params: 383,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cJ-OD0Cpdel",
        "colab_type": "text"
      },
      "source": [
        "Para crear la segunda definimos una estructura más compleja, formada por una consecución de tres pares de capas Dropout y LSTM. Además, en este caso se usan los pesos preentrenados de GloVe que hemos cargado anteriormente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD1lFN2Tpdvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "7923207a-740d-452d-e893-27099c55dad2"
      },
      "source": [
        "# Hiperparámetros de la red E1\n",
        "embed_dim = 50\n",
        "lstm_out = 16\n",
        "\n",
        "# Inicialización\n",
        "model1 = Sequential()\n",
        "# Capa de entrada\n",
        "model1.add(Embedding(vocab, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
        "# Capas Dropout y LSTM\n",
        "model1.add(SpatialDropout1D(0.3))\n",
        "model1.add(LSTM(lstm_out*3, dropout=0.1, return_sequences=True))\n",
        "model1.add(SpatialDropout1D(0.2))\n",
        "model1.add(LSTM(lstm_out*2, dropout=0.2, return_sequences=True))\n",
        "model1.add(SpatialDropout1D(0.1))\n",
        "model1.add(LSTM(lstm_out, dropout=0.1))\n",
        "# Capa con dos salidas\n",
        "#   Salida 0: emociones negativas\n",
        "#   Salida 1: emociones positivas\n",
        "model1.add(Dense(2,activation='softmax'))\n",
        "# Compilamos el modelo\n",
        "model1.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# Mostramos su información\n",
        "print(model1.summary())\n",
        "\n",
        "# Guardamos el modelo y sus hiperparámetros en sus listas\n",
        "models.append(model1)\n",
        "batch_sizes.append(64)\n",
        "n_registers.append(10000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 600, 50)           4206100   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 600, 50)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 600, 48)           19008     \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_3 (Spatial (None, 600, 48)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 600, 32)           10368     \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_4 (Spatial (None, 600, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 16)                3136      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 4,238,646\n",
            "Trainable params: 4,238,646\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6N-hpQIqIp-",
        "colab_type": "text"
      },
      "source": [
        "El tercer modelo tiene una estructura muy similar al primero, pero en este se usan también los pesos preentrenados de GloVe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR2gNb7VqMGf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "9f0daa8f-afd6-4ee9-b653-36d1bcfd3b46"
      },
      "source": [
        "# Hiperparámetros de la red E2\n",
        "embed_dim = 50\n",
        "lstm_out = 196\n",
        "\n",
        "# Inicialización\n",
        "model2 = Sequential()\n",
        "# Capa de entrada\n",
        "model2.add(Embedding(vocab, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
        "# Capa Dropout\n",
        "model2.add(SpatialDropout1D(0.4))\n",
        "# Capa LSTM\n",
        "model2.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "# Capa con dos salidas\n",
        "#   Salida 0: emociones negativas\n",
        "#   Salida 1: emociones positivas\n",
        "model2.add(Dense(2,activation='softmax'))\n",
        "# Compilamos el modelo\n",
        "model2.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# Mostramos su información\n",
        "print(model2.summary())\n",
        "\n",
        "# Guardamos el modelo en la lista de modelos, al igual que el tamaño de batch y el número de registros\n",
        "models.append(model2)\n",
        "batch_sizes.append(128)\n",
        "n_registers.append(8000)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 600, 50)           4206100   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_5 (Spatial (None, 600, 50)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 196)               193648    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 394       \n",
            "=================================================================\n",
            "Total params: 4,400,142\n",
            "Trainable params: 4,400,142\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHMzks5XrCF9",
        "colab_type": "text"
      },
      "source": [
        "El último modelo cuenta con la misma estructura que el segundo, pero en este caso no se usan GloVe y los pesos se entrenarán directamente con el resto del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZIyq6PDrD8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "af1c5e76-b41b-421a-b21e-7c47d33fe645"
      },
      "source": [
        "# Hiperparámetros\n",
        "embed_dim = 50\n",
        "lstm_out = 16\n",
        "\n",
        "# Inicialización\n",
        "model3 = Sequential()\n",
        "# Capa de entrada\n",
        "model3.add(Embedding(max_features, embed_dim,input_length = max_length))\n",
        "# Capas Dropout y LSTM\n",
        "model3.add(SpatialDropout1D(0.3))\n",
        "model3.add(LSTM(lstm_out*3, dropout=0.1, return_sequences=True))\n",
        "model3.add(SpatialDropout1D(0.2))\n",
        "model3.add(LSTM(lstm_out*2, dropout=0.2, return_sequences=True))\n",
        "model3.add(SpatialDropout1D(0.1))\n",
        "model3.add(LSTM(lstm_out, dropout=0.1))\n",
        "# Capas de salida\n",
        "model3.add(Dense(2,activation='softmax'))\n",
        "# Compilamos el modelo\n",
        "model3.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# Mostramos su información\n",
        "print(model3.summary())\n",
        "\n",
        "# Guardamos el modelo y sus hiperparámetros en sus listas\n",
        "models.append(model3)\n",
        "batch_sizes.append(64)\n",
        "n_registers.append(10000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 600, 50)           50000     \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_6 (Spatial (None, 600, 50)           0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 600, 48)           19008     \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_7 (Spatial (None, 600, 48)           0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 600, 32)           10368     \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_8 (Spatial (None, 600, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 16)                3136      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 82,546\n",
            "Trainable params: 82,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMz-2_f3kZO-",
        "colab_type": "text"
      },
      "source": [
        "Ahora pasamos a entrenar cada una de las redes. Para ello, se ha creado una función con el fin de automatizar el proceso, que comienza adaptando los datos de entrada y desordenando el conjunto de training para evitar que cada red se entrene con el mismo conjunto que las demás. Posteriormente hay que convertir cada palabra de las reseñas a conjuntos numéricos, y también obtener la puntuación de cada una.\n",
        "\n",
        "Tras entrenar el modelo, se guarda en nuestra cuenta de Google Drive para su posterior análisis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrJshEHznFWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_models(tokenizer, n_registers, models, batch_sizes):\n",
        "  \n",
        "  # Para cada uno de los modelos\n",
        "  for i in range(0, len(models)):\n",
        "    \n",
        "    # Desordenamos los registros del dataset de manera aleatoria\n",
        "    train.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    # Nos quedamos con un número determinado de registros\n",
        "    data_t = train.iloc[:n_registers[i]]\n",
        "    print(data_t.count())\n",
        "    \n",
        "    # Convertimos cada palabra a secuencias de números\n",
        "    X = tokenizer.texts_to_sequences(data_t['Review'].values)\n",
        "    X = pad_sequences(X, maxlen=max_length)\n",
        "    \n",
        "    # Obtenemos las salidas esperadas \n",
        "    Y = pd.get_dummies(data_t['Reviewer_Score']).values\n",
        "    \n",
        "    # Entrenamos el modelo\n",
        "    print(\"Training NN \"+str(i))    \n",
        "    models[i].fit(X, Y, epochs = 6, batch_size = batch_sizes[i], verbose = 2)\n",
        "    \n",
        "    # Guardamos el modelo en nuestro Google Drive\n",
        "    models[i].save('/content/gdrive/My Drive/E'+str(i)+'.h5')\n",
        "    print(\"Saved NN \"+str(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS12uXFKsWj-",
        "colab_type": "text"
      },
      "source": [
        "Llamamos a la función con los modelos compilados anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cICljCQokYpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b0bd07a-a2ea-4cb7-ab1c-5c6374e14995"
      },
      "source": [
        "create_models(tokenizer, n_registers, models, batch_sizes)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review            8000\n",
            "Reviewer_Score    8000\n",
            "dtype: int64\n",
            "Training NN 0\n",
            "Epoch 1/6\n",
            " - 379s - loss: 0.4809 - acc: 0.7761\n",
            "Epoch 2/6\n",
            " - 378s - loss: 0.4679 - acc: 0.7834\n",
            "Epoch 3/6\n",
            " - 381s - loss: 0.4497 - acc: 0.7891\n",
            "Epoch 4/6\n",
            " - 380s - loss: 0.4373 - acc: 0.8016\n",
            "Epoch 5/6\n",
            " - 381s - loss: 0.4262 - acc: 0.8056\n",
            "Epoch 6/6\n",
            " - 380s - loss: 0.4231 - acc: 0.8073\n",
            "Saved NN 0\n",
            "Review            10000\n",
            "Reviewer_Score    10000\n",
            "dtype: int64\n",
            "Training NN 1\n",
            "Epoch 1/6\n",
            " - 244s - loss: 0.5803 - acc: 0.7308\n",
            "Epoch 2/6\n",
            " - 241s - loss: 0.5524 - acc: 0.7358\n",
            "Epoch 3/6\n",
            " - 241s - loss: 0.5297 - acc: 0.7426\n",
            "Epoch 4/6\n",
            " - 240s - loss: 0.5221 - acc: 0.7420\n",
            "Epoch 5/6\n",
            " - 240s - loss: 0.5099 - acc: 0.7563\n",
            "Epoch 6/6\n",
            " - 242s - loss: 0.5000 - acc: 0.7582\n",
            "Saved NN 1\n",
            "Review            8000\n",
            "Reviewer_Score    8000\n",
            "dtype: int64\n",
            "Training NN 2\n",
            "Epoch 1/6\n",
            " - 309s - loss: 0.5776 - acc: 0.7326\n",
            "Epoch 2/6\n",
            " - 306s - loss: 0.5599 - acc: 0.7355\n",
            "Epoch 3/6\n",
            " - 306s - loss: 0.5456 - acc: 0.7399\n",
            "Epoch 4/6\n",
            " - 307s - loss: 0.5322 - acc: 0.7491\n",
            "Epoch 5/6\n",
            " - 307s - loss: 0.5218 - acc: 0.7490\n",
            "Epoch 6/6\n",
            " - 307s - loss: 0.5161 - acc: 0.7580\n",
            "Saved NN 2\n",
            "Review            10000\n",
            "Reviewer_Score    10000\n",
            "dtype: int64\n",
            "Training NN 3\n",
            "Epoch 1/6\n",
            " - 237s - loss: 0.5685 - acc: 0.7354\n",
            "Epoch 2/6\n",
            " - 232s - loss: 0.5116 - acc: 0.7569\n",
            "Epoch 3/6\n",
            " - 232s - loss: 0.4902 - acc: 0.7737\n",
            "Epoch 4/6\n",
            " - 232s - loss: 0.4717 - acc: 0.7802\n",
            "Epoch 5/6\n",
            " - 232s - loss: 0.4681 - acc: 0.7848\n",
            "Epoch 6/6\n",
            " - 231s - loss: 0.4543 - acc: 0.7973\n",
            "Saved NN 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXrKgEtlYoAE",
        "colab_type": "text"
      },
      "source": [
        "Llegados a este punto, es hora de aplicar el conjunto de test sobre cada una de nuestras redes. De nuevo y por comodidad, se ha definido una función que nos permitirá testear también el ensemble sumando aquellos campos que no son iguales al comparar las predicciones con los resultados esperados para el conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Py3II2tYm6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_error(model, tokenizer, test) -> np.float64:\n",
        "  \n",
        "  # Convertimos el texto del conjunto de test para que pueda ser evaluado por los modelos\n",
        "  x_test = tokenizer.texts_to_sequences(test['Review'].values)\n",
        "  x_test = pad_sequences(x_test, maxlen=max_length)\n",
        "  y_test = test['Reviewer_Score'].values\n",
        "  \n",
        "  # Predecimos los resultados\n",
        "  pred = model.predict(x_test, batch_size = 128)\n",
        "  # Nos quedamos con la salida de la red que tiene mayor valor\n",
        "  pred = np.argmax(pred, axis=1)\n",
        "  # Calculamos el error y lo devolvemos, al igual que el conjunto de predicciones\n",
        "  error = np.sum(np.not_equal(pred, y_test)) / y_test.size   \n",
        "  return error, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un1ZUDR-YaCn",
        "colab_type": "text"
      },
      "source": [
        "Llegados a este punto, cargaremos los modelos que hemos entrenado, para obtener sus errores y predicciones asociadas al conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oRxiZUeYaiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cargamos los modelos\n",
        "models3 = []\n",
        "models3.append(load_model('/content/gdrive/My Drive/E0.h5'))\n",
        "models3.append(load_model('/content/gdrive/My Drive/E1.h5'))\n",
        "models3.append(load_model('/content/gdrive/My Drive/E2.h5'))\n",
        "models3.append(load_model('/content/gdrive/My Drive/E3.h5'))\n",
        "\n",
        "predictions2 = []\n",
        "errors2 = []\n",
        "for x in range(0,4):\n",
        "  # Obtenemos el error y las predicciones\n",
        "  err, pred = evaluate_error(models3[x],tokenizer, test)\n",
        "  predictions2.append(pred)\n",
        "  errors2.append(err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y2q2_xSY49i",
        "colab_type": "text"
      },
      "source": [
        "Los errores obtenidos para cada uno de los modelos son muy similares entre si. Esto se debe principalmente a que no han sido entrenados en profundidad, debido al enorme gasto de recursos que supone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHGlHK9VY4Xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "54b8a4e7-5cd6-4ebb-80ca-35575aa42a16"
      },
      "source": [
        "errors2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.24933333333333332,\n",
              " 0.23266666666666666,\n",
              " 0.24766666666666667,\n",
              " 0.24066666666666667]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hz2JZkcY9YW",
        "colab_type": "text"
      },
      "source": [
        "Teniendo cada una de las predicciones, podemos definir la lógica del ensemble y generar una predicción final. Por simplicidad definiremos el sistema de voto por la mayoría, y en caso de empate se decidirá el resultado de forma aleatoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNJgpIgMY9gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "\n",
        "size = len(predictions2[0])\n",
        "pos = 0\n",
        "n_models = len(predictions2)\n",
        "fp = []\n",
        "final_pred = []\n",
        "\n",
        "for x in range(0,size):\n",
        "  for i in range(0,n_models):\n",
        "    if predictions2[i][x] == 1:\n",
        "      # La variable pos cuenta el número de resultados etiquetados como positivos\n",
        "      pos = pos + 1\n",
        "  # Si el número de positivos es mayor que 2, el resultado final es 1\n",
        "  if pos > 2:\n",
        "    final_pred.append(1)\n",
        "  # Si hay empate, se decide aleatoriamente\n",
        "  elif pos == 2:\n",
        "    final_pred.append(randint(0,1))\n",
        "  # En otro caso, el resultado final es 0\n",
        "  else:\n",
        "    final_pred.append(0)\n",
        "  fp.append(pos)\n",
        "  pos = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZE40TcZZATt",
        "colab_type": "text"
      },
      "source": [
        "Y ahora pasamos a calcular el error del ensemble sobre el conjunto de test. Vemos que la capacidad de acierto no mejora apenas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SByMzx8KZAb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "160e66d1-43ac-4815-a2be-5f851c0567b3"
      },
      "source": [
        "y_test = test['Reviewer_Score'].values\n",
        "error = np.sum(np.not_equal(final_pred, y_test)) / y_test.size\n",
        "error"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzszMv870OrX",
        "colab_type": "text"
      },
      "source": [
        "Podemos analizar más en detalle que está sucediendo si imprimimos por pantalla las predicciones de los modelos individuales y la predicción esperada. También se ha incluido el número de reseña."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl8PPIUGbAA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "equals = np.not_equal(final_pred, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD4d-e5m0mGU",
        "colab_type": "text"
      },
      "source": [
        "Tal y como podemos apreciar, la gran mayoría de los errores provienen de predicciones en las que todos los modelos fallan. Esto es un indicador de que este tipo de ensembles simples no son del todo indicados para abordar problemas de esta clase con redes LSTM, a diferencia de los buenos resultados obtenidos en otros casos de naturaleza distinta al análisis de textos.\n",
        "\n",
        "El principal problema radica en la complejidad en el entrenamiento. Como hemos comentado anteriormente, este tipo de redes necesitan de una gran cantidad de recursos, como por ejemplo memoria RAM, y tardan un tiempo significativamente elevado en conseguir calidad en los modelos a diferencia de otro tipo de problemas complejos como el reconocimiento de objetos en imágenes.\n",
        "\n",
        "Sin embargo, introducir una arquitectura basada en ensemble siempre es beneficioso ya que aumenta la robustez del modelo, dado que la probabilidad de fallo es siempre menor usando un conjunto de modelos que con uno sólo. Algunas de las soluciones para conseguir mayor capacidad de acierto en el ensemble podrían ser:\n",
        "\n",
        "\n",
        "*   Utilizar más recursos computacionales a la hora de entrenar el modelo.\n",
        "*   Introducir nuevos modelos individuales.\n",
        "*   Introducir otros tipos de modelos individuales, como máquinas de soporte vectorial (SVMs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt-CnorxdbFM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c86cd670-b293-4962-8644-7b937cbf89d9"
      },
      "source": [
        "print(\"E0, E1, E2, E3, y\")\n",
        "for i in range(0,equals.size):\n",
        "  if equals[i] == True:\n",
        "    print(str(predictions2[0][i])+\", \"+str(predictions2[1][i])+\", \"+str(predictions2[2][i])+\", \"+str(predictions2[3][i])+\", \"+str(y_test[i])+\"     \"+str(i))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E0, E1, E2, E3, y\n",
            "1, 1, 1, 1, 0     0\n",
            "1, 1, 1, 1, 0     10\n",
            "1, 1, 1, 1, 0     12\n",
            "1, 1, 1, 1, 0     15\n",
            "1, 0, 0, 1, 1     18\n",
            "1, 1, 1, 1, 0     23\n",
            "1, 1, 1, 1, 0     25\n",
            "1, 0, 1, 0, 1     27\n",
            "1, 1, 1, 1, 0     31\n",
            "1, 0, 1, 0, 0     34\n",
            "1, 1, 1, 1, 0     39\n",
            "1, 1, 1, 1, 0     42\n",
            "1, 1, 1, 1, 0     43\n",
            "1, 0, 0, 1, 1     48\n",
            "1, 1, 1, 1, 0     57\n",
            "1, 0, 1, 1, 0     58\n",
            "1, 0, 1, 0, 0     59\n",
            "1, 1, 1, 1, 0     64\n",
            "1, 1, 1, 1, 0     66\n",
            "1, 1, 1, 1, 0     74\n",
            "1, 1, 1, 1, 0     78\n",
            "0, 0, 1, 1, 0     80\n",
            "1, 1, 1, 1, 0     81\n",
            "0, 0, 0, 0, 1     82\n",
            "1, 1, 1, 1, 0     84\n",
            "1, 0, 1, 1, 0     85\n",
            "1, 1, 1, 1, 0     95\n",
            "0, 0, 0, 0, 1     96\n",
            "1, 1, 1, 1, 0     102\n",
            "1, 1, 1, 1, 0     108\n",
            "1, 0, 1, 1, 0     110\n",
            "1, 0, 0, 1, 0     124\n",
            "0, 0, 1, 0, 1     130\n",
            "1, 1, 1, 1, 0     134\n",
            "1, 1, 1, 1, 0     136\n",
            "1, 1, 1, 1, 0     137\n",
            "1, 1, 1, 1, 0     145\n",
            "1, 0, 0, 1, 1     155\n",
            "1, 0, 0, 1, 1     156\n",
            "1, 1, 1, 0, 0     158\n",
            "0, 0, 0, 0, 1     160\n",
            "1, 0, 0, 0, 1     161\n",
            "1, 1, 1, 1, 0     171\n",
            "1, 0, 0, 0, 1     172\n",
            "1, 1, 1, 1, 0     179\n",
            "1, 0, 1, 1, 0     185\n",
            "0, 0, 0, 0, 1     186\n",
            "1, 1, 1, 1, 0     190\n",
            "1, 0, 0, 1, 0     203\n",
            "1, 1, 1, 1, 0     206\n",
            "1, 1, 1, 1, 0     213\n",
            "1, 1, 1, 0, 0     214\n",
            "1, 1, 1, 1, 0     217\n",
            "1, 1, 1, 1, 0     225\n",
            "1, 1, 1, 1, 0     228\n",
            "1, 1, 1, 1, 0     231\n",
            "1, 1, 1, 1, 0     232\n",
            "1, 1, 1, 1, 0     234\n",
            "1, 1, 1, 1, 0     236\n",
            "1, 1, 1, 1, 0     246\n",
            "0, 0, 0, 0, 1     250\n",
            "1, 1, 1, 1, 0     254\n",
            "0, 1, 1, 0, 0     264\n",
            "1, 1, 1, 1, 0     270\n",
            "0, 1, 1, 0, 1     271\n",
            "0, 1, 1, 1, 0     277\n",
            "1, 1, 1, 1, 0     279\n",
            "1, 0, 1, 1, 0     280\n",
            "1, 1, 1, 1, 0     282\n",
            "1, 1, 1, 1, 0     283\n",
            "1, 1, 1, 1, 0     289\n",
            "1, 1, 1, 1, 0     290\n",
            "1, 1, 1, 1, 0     297\n",
            "1, 1, 1, 1, 0     307\n",
            "1, 0, 1, 1, 0     311\n",
            "1, 0, 1, 1, 0     312\n",
            "0, 0, 0, 1, 1     320\n",
            "1, 0, 1, 1, 0     324\n",
            "1, 1, 1, 0, 0     327\n",
            "1, 1, 1, 1, 0     330\n",
            "1, 1, 1, 1, 0     335\n",
            "1, 1, 1, 1, 0     342\n",
            "1, 1, 1, 1, 0     350\n",
            "0, 0, 0, 0, 1     351\n",
            "1, 1, 1, 1, 0     352\n",
            "1, 1, 1, 1, 0     356\n",
            "0, 0, 1, 1, 0     360\n",
            "0, 0, 1, 1, 1     368\n",
            "1, 1, 1, 1, 0     370\n",
            "0, 0, 1, 1, 0     371\n",
            "1, 1, 1, 1, 0     379\n",
            "1, 1, 1, 1, 0     380\n",
            "1, 1, 1, 1, 0     383\n",
            "1, 1, 1, 1, 0     389\n",
            "0, 0, 1, 0, 1     391\n",
            "0, 0, 1, 0, 1     393\n",
            "0, 0, 1, 0, 1     394\n",
            "1, 1, 1, 1, 0     395\n",
            "0, 1, 0, 1, 0     396\n",
            "0, 0, 0, 1, 1     401\n",
            "0, 1, 0, 0, 1     403\n",
            "0, 0, 1, 0, 1     404\n",
            "1, 1, 1, 1, 0     411\n",
            "1, 0, 1, 1, 0     415\n",
            "1, 1, 1, 1, 0     420\n",
            "1, 1, 1, 1, 0     422\n",
            "1, 1, 1, 1, 0     423\n",
            "1, 1, 1, 1, 0     429\n",
            "1, 1, 1, 1, 0     431\n",
            "0, 0, 1, 0, 1     437\n",
            "0, 1, 1, 0, 1     442\n",
            "0, 0, 0, 0, 1     444\n",
            "0, 0, 1, 1, 0     447\n",
            "1, 0, 1, 1, 0     451\n",
            "1, 1, 1, 1, 0     453\n",
            "1, 1, 1, 1, 0     458\n",
            "0, 0, 1, 0, 1     459\n",
            "1, 0, 1, 1, 0     463\n",
            "1, 1, 1, 1, 0     469\n",
            "1, 1, 1, 1, 0     473\n",
            "0, 0, 1, 0, 1     475\n",
            "1, 1, 1, 1, 0     476\n",
            "1, 1, 1, 1, 0     478\n",
            "1, 0, 1, 0, 0     479\n",
            "0, 0, 1, 0, 1     481\n",
            "1, 1, 0, 1, 0     482\n",
            "1, 0, 1, 1, 0     484\n",
            "0, 0, 0, 0, 1     490\n",
            "1, 1, 1, 1, 0     492\n",
            "1, 1, 1, 1, 0     493\n",
            "1, 1, 0, 0, 0     495\n",
            "0, 0, 0, 0, 1     496\n",
            "1, 1, 1, 1, 0     501\n",
            "1, 1, 1, 1, 0     504\n",
            "0, 1, 1, 0, 1     505\n",
            "1, 1, 1, 1, 0     511\n",
            "1, 1, 1, 1, 0     514\n",
            "1, 0, 0, 1, 1     517\n",
            "1, 1, 1, 1, 0     519\n",
            "1, 0, 1, 0, 1     522\n",
            "1, 1, 1, 1, 0     529\n",
            "1, 1, 1, 1, 0     539\n",
            "1, 0, 0, 0, 1     541\n",
            "1, 1, 1, 1, 0     542\n",
            "1, 1, 1, 1, 0     549\n",
            "1, 1, 1, 1, 0     550\n",
            "1, 1, 0, 1, 0     551\n",
            "1, 1, 1, 1, 0     557\n",
            "1, 0, 0, 1, 1     568\n",
            "1, 1, 1, 1, 0     570\n",
            "1, 0, 0, 0, 1     573\n",
            "1, 1, 1, 1, 0     576\n",
            "1, 1, 1, 1, 0     579\n",
            "0, 1, 1, 1, 0     587\n",
            "0, 0, 0, 0, 1     588\n",
            "1, 1, 1, 1, 0     595\n",
            "1, 1, 1, 0, 0     614\n",
            "1, 1, 1, 1, 0     615\n",
            "1, 0, 0, 0, 1     625\n",
            "1, 1, 1, 1, 0     627\n",
            "0, 1, 1, 1, 0     628\n",
            "1, 1, 1, 1, 0     633\n",
            "0, 0, 0, 0, 1     635\n",
            "0, 0, 1, 0, 1     638\n",
            "1, 0, 1, 1, 0     640\n",
            "1, 1, 1, 1, 0     644\n",
            "1, 1, 1, 1, 0     652\n",
            "1, 1, 1, 1, 0     656\n",
            "1, 0, 1, 0, 0     666\n",
            "1, 0, 1, 1, 0     673\n",
            "0, 1, 1, 1, 0     688\n",
            "1, 1, 1, 1, 0     689\n",
            "1, 1, 1, 1, 0     692\n",
            "1, 0, 0, 1, 1     696\n",
            "1, 1, 0, 1, 0     699\n",
            "1, 1, 1, 1, 0     700\n",
            "1, 1, 1, 0, 0     705\n",
            "1, 1, 1, 1, 0     706\n",
            "1, 0, 1, 1, 0     710\n",
            "1, 0, 1, 1, 0     720\n",
            "1, 0, 1, 1, 0     730\n",
            "0, 0, 1, 0, 1     732\n",
            "1, 1, 1, 0, 0     733\n",
            "1, 0, 0, 1, 0     736\n",
            "0, 0, 1, 1, 1     738\n",
            "1, 1, 1, 1, 0     739\n",
            "0, 0, 1, 0, 1     740\n",
            "1, 1, 1, 1, 0     741\n",
            "1, 1, 1, 1, 0     742\n",
            "0, 1, 1, 0, 0     756\n",
            "1, 0, 1, 0, 0     760\n",
            "1, 0, 1, 1, 0     762\n",
            "0, 0, 1, 0, 1     764\n",
            "0, 0, 1, 0, 1     772\n",
            "1, 1, 1, 1, 0     775\n",
            "0, 0, 0, 1, 1     777\n",
            "1, 1, 1, 1, 0     793\n",
            "0, 0, 0, 0, 1     795\n",
            "0, 0, 0, 1, 1     802\n",
            "1, 0, 1, 0, 0     803\n",
            "0, 0, 1, 1, 0     815\n",
            "1, 1, 1, 1, 0     817\n",
            "0, 0, 0, 0, 1     819\n",
            "0, 1, 1, 0, 1     825\n",
            "0, 0, 1, 1, 0     826\n",
            "1, 1, 1, 1, 0     832\n",
            "1, 1, 0, 0, 0     845\n",
            "1, 0, 1, 1, 0     849\n",
            "1, 0, 0, 1, 0     853\n",
            "0, 0, 1, 0, 1     856\n",
            "1, 0, 1, 1, 0     859\n",
            "1, 1, 1, 1, 0     867\n",
            "1, 1, 1, 1, 0     870\n",
            "1, 1, 1, 1, 0     873\n",
            "1, 1, 1, 1, 0     875\n",
            "1, 1, 1, 1, 0     877\n",
            "1, 1, 1, 1, 0     879\n",
            "1, 1, 1, 1, 0     880\n",
            "0, 1, 1, 1, 0     881\n",
            "1, 1, 1, 1, 0     889\n",
            "1, 1, 1, 1, 0     891\n",
            "1, 1, 1, 1, 0     892\n",
            "1, 0, 1, 1, 0     910\n",
            "1, 1, 1, 1, 0     911\n",
            "1, 1, 1, 1, 0     912\n",
            "1, 1, 1, 1, 0     915\n",
            "1, 0, 1, 1, 0     920\n",
            "1, 1, 1, 1, 0     921\n",
            "1, 1, 1, 1, 0     922\n",
            "1, 1, 1, 1, 0     935\n",
            "0, 0, 1, 1, 0     939\n",
            "1, 0, 1, 0, 0     946\n",
            "1, 1, 1, 1, 0     948\n",
            "0, 0, 0, 1, 1     953\n",
            "1, 1, 1, 1, 0     974\n",
            "1, 1, 1, 1, 0     978\n",
            "0, 0, 0, 1, 1     980\n",
            "1, 1, 1, 1, 0     985\n",
            "1, 1, 1, 1, 0     989\n",
            "0, 1, 0, 0, 1     990\n",
            "1, 0, 0, 1, 0     1004\n",
            "1, 1, 1, 1, 0     1019\n",
            "1, 1, 1, 1, 0     1023\n",
            "1, 1, 1, 1, 0     1029\n",
            "1, 1, 1, 1, 0     1032\n",
            "1, 0, 1, 1, 0     1033\n",
            "0, 1, 1, 0, 1     1041\n",
            "1, 0, 1, 0, 0     1054\n",
            "1, 0, 0, 0, 1     1063\n",
            "1, 1, 1, 1, 0     1069\n",
            "1, 1, 1, 1, 0     1076\n",
            "1, 1, 1, 0, 0     1086\n",
            "1, 0, 1, 1, 0     1090\n",
            "1, 1, 1, 1, 0     1092\n",
            "1, 1, 1, 1, 0     1105\n",
            "0, 0, 0, 0, 1     1108\n",
            "1, 0, 0, 1, 0     1109\n",
            "1, 1, 1, 1, 0     1112\n",
            "0, 1, 1, 0, 1     1113\n",
            "1, 1, 1, 1, 0     1115\n",
            "1, 0, 1, 1, 0     1122\n",
            "1, 1, 1, 1, 0     1123\n",
            "0, 0, 1, 1, 0     1125\n",
            "1, 1, 1, 1, 0     1126\n",
            "1, 1, 1, 1, 0     1131\n",
            "1, 1, 0, 1, 0     1133\n",
            "1, 0, 1, 1, 0     1135\n",
            "0, 0, 0, 0, 1     1141\n",
            "1, 1, 1, 1, 0     1142\n",
            "0, 0, 1, 0, 1     1144\n",
            "1, 1, 1, 1, 0     1146\n",
            "1, 1, 1, 1, 0     1148\n",
            "1, 1, 1, 1, 0     1158\n",
            "1, 1, 1, 1, 0     1169\n",
            "1, 1, 1, 1, 0     1173\n",
            "1, 1, 1, 1, 0     1179\n",
            "1, 1, 1, 1, 0     1183\n",
            "1, 0, 0, 0, 1     1188\n",
            "1, 1, 0, 1, 0     1189\n",
            "1, 1, 1, 1, 0     1191\n",
            "1, 1, 1, 1, 0     1204\n",
            "1, 1, 1, 1, 0     1215\n",
            "1, 1, 1, 1, 0     1219\n",
            "0, 1, 1, 1, 0     1224\n",
            "1, 1, 1, 1, 0     1233\n",
            "1, 0, 0, 1, 0     1236\n",
            "1, 1, 1, 1, 0     1240\n",
            "0, 1, 1, 0, 0     1245\n",
            "1, 1, 1, 1, 0     1251\n",
            "1, 1, 1, 1, 0     1252\n",
            "1, 0, 0, 1, 0     1255\n",
            "1, 1, 1, 1, 0     1256\n",
            "1, 1, 1, 1, 0     1257\n",
            "1, 1, 1, 1, 0     1260\n",
            "1, 0, 1, 0, 0     1266\n",
            "1, 1, 1, 1, 0     1267\n",
            "1, 1, 1, 0, 0     1271\n",
            "0, 0, 0, 0, 1     1275\n",
            "1, 1, 1, 1, 0     1284\n",
            "0, 0, 0, 0, 1     1287\n",
            "1, 1, 1, 1, 0     1291\n",
            "1, 1, 1, 1, 0     1293\n",
            "1, 0, 1, 1, 0     1294\n",
            "1, 1, 1, 1, 0     1303\n",
            "1, 0, 0, 0, 1     1304\n",
            "1, 0, 1, 0, 0     1307\n",
            "1, 1, 1, 1, 0     1308\n",
            "1, 1, 1, 1, 0     1310\n",
            "0, 1, 1, 1, 0     1315\n",
            "1, 1, 1, 1, 0     1317\n",
            "1, 1, 1, 1, 0     1325\n",
            "1, 0, 0, 1, 1     1331\n",
            "0, 0, 1, 1, 1     1334\n",
            "1, 0, 0, 0, 1     1337\n",
            "1, 0, 1, 1, 0     1345\n",
            "1, 1, 1, 1, 0     1346\n",
            "1, 1, 1, 1, 0     1349\n",
            "1, 1, 1, 1, 0     1354\n",
            "1, 0, 1, 0, 1     1357\n",
            "1, 1, 1, 1, 0     1358\n",
            "1, 0, 0, 0, 1     1361\n",
            "1, 1, 1, 1, 0     1365\n",
            "1, 1, 1, 1, 0     1366\n",
            "1, 1, 1, 1, 0     1368\n",
            "1, 1, 1, 1, 0     1369\n",
            "1, 1, 1, 1, 0     1370\n",
            "1, 1, 1, 1, 0     1374\n",
            "1, 1, 1, 1, 0     1378\n",
            "0, 0, 1, 0, 1     1380\n",
            "1, 1, 1, 1, 0     1381\n",
            "1, 1, 1, 1, 0     1388\n",
            "1, 0, 0, 0, 1     1393\n",
            "1, 1, 1, 1, 0     1394\n",
            "1, 1, 1, 0, 0     1395\n",
            "1, 1, 1, 1, 0     1397\n",
            "1, 1, 1, 1, 0     1403\n",
            "1, 1, 1, 1, 0     1404\n",
            "1, 1, 1, 1, 0     1405\n",
            "1, 1, 1, 1, 0     1411\n",
            "1, 0, 1, 0, 0     1426\n",
            "0, 0, 1, 1, 1     1427\n",
            "1, 1, 1, 1, 0     1428\n",
            "1, 0, 1, 0, 1     1429\n",
            "1, 1, 1, 1, 0     1430\n",
            "1, 0, 1, 0, 0     1431\n",
            "1, 1, 1, 1, 0     1439\n",
            "1, 1, 1, 1, 0     1445\n",
            "0, 0, 0, 0, 1     1448\n",
            "1, 1, 1, 1, 0     1455\n",
            "1, 0, 1, 1, 0     1459\n",
            "1, 1, 1, 1, 0     1461\n",
            "1, 0, 1, 1, 0     1466\n",
            "0, 0, 1, 0, 1     1469\n",
            "0, 1, 1, 0, 0     1473\n",
            "1, 1, 1, 1, 0     1474\n",
            "1, 1, 1, 1, 0     1476\n",
            "1, 1, 1, 1, 0     1485\n",
            "1, 1, 1, 1, 0     1489\n",
            "1, 1, 1, 1, 0     1492\n",
            "1, 1, 1, 1, 0     1498\n",
            "1, 1, 1, 1, 0     1502\n",
            "0, 0, 1, 0, 1     1507\n",
            "0, 0, 1, 0, 1     1511\n",
            "0, 0, 0, 0, 1     1513\n",
            "1, 0, 1, 1, 0     1518\n",
            "1, 1, 1, 0, 0     1519\n",
            "0, 0, 1, 1, 1     1527\n",
            "1, 1, 1, 1, 0     1529\n",
            "1, 1, 1, 1, 0     1530\n",
            "1, 1, 1, 1, 0     1532\n",
            "1, 1, 1, 1, 0     1534\n",
            "1, 1, 1, 1, 0     1542\n",
            "1, 1, 1, 1, 0     1550\n",
            "1, 1, 1, 1, 0     1551\n",
            "1, 1, 1, 1, 0     1552\n",
            "0, 0, 0, 1, 1     1553\n",
            "1, 1, 1, 1, 0     1554\n",
            "1, 0, 0, 1, 1     1555\n",
            "1, 1, 1, 1, 0     1560\n",
            "1, 1, 1, 1, 0     1561\n",
            "1, 1, 1, 1, 0     1564\n",
            "1, 0, 0, 1, 1     1568\n",
            "1, 0, 1, 1, 0     1570\n",
            "1, 1, 1, 1, 0     1586\n",
            "1, 1, 0, 0, 0     1589\n",
            "1, 0, 1, 1, 0     1590\n",
            "1, 0, 0, 1, 0     1598\n",
            "0, 1, 1, 1, 0     1600\n",
            "0, 1, 0, 0, 1     1618\n",
            "1, 1, 1, 1, 0     1623\n",
            "1, 1, 1, 1, 0     1628\n",
            "1, 0, 1, 0, 1     1639\n",
            "1, 1, 1, 1, 0     1647\n",
            "1, 1, 1, 1, 0     1648\n",
            "1, 0, 1, 1, 0     1660\n",
            "1, 0, 1, 1, 0     1665\n",
            "1, 1, 1, 1, 0     1666\n",
            "1, 0, 1, 1, 0     1671\n",
            "0, 0, 1, 0, 1     1676\n",
            "1, 1, 1, 1, 0     1681\n",
            "1, 1, 1, 1, 0     1682\n",
            "1, 0, 1, 1, 0     1696\n",
            "0, 0, 1, 0, 1     1699\n",
            "1, 1, 1, 1, 0     1701\n",
            "1, 0, 1, 0, 0     1705\n",
            "1, 1, 1, 1, 0     1706\n",
            "1, 1, 1, 1, 0     1715\n",
            "0, 1, 1, 1, 0     1716\n",
            "1, 1, 1, 1, 0     1720\n",
            "1, 1, 1, 1, 0     1721\n",
            "1, 1, 1, 1, 0     1728\n",
            "1, 1, 1, 1, 0     1729\n",
            "0, 0, 1, 0, 1     1730\n",
            "1, 0, 1, 0, 1     1732\n",
            "1, 1, 0, 1, 0     1736\n",
            "1, 1, 0, 0, 0     1742\n",
            "1, 1, 1, 1, 0     1744\n",
            "1, 0, 1, 0, 0     1746\n",
            "1, 1, 1, 1, 0     1759\n",
            "1, 0, 0, 0, 1     1762\n",
            "0, 1, 1, 0, 0     1763\n",
            "0, 0, 0, 0, 1     1765\n",
            "1, 1, 1, 1, 0     1773\n",
            "0, 0, 0, 0, 1     1777\n",
            "1, 1, 1, 0, 0     1782\n",
            "0, 0, 0, 0, 1     1787\n",
            "0, 1, 1, 0, 1     1789\n",
            "0, 0, 0, 0, 1     1791\n",
            "0, 1, 1, 0, 0     1799\n",
            "1, 1, 1, 1, 0     1803\n",
            "1, 1, 1, 1, 0     1809\n",
            "1, 1, 1, 1, 0     1813\n",
            "1, 1, 1, 1, 0     1817\n",
            "1, 0, 1, 1, 0     1820\n",
            "0, 0, 1, 0, 1     1821\n",
            "1, 1, 1, 1, 0     1825\n",
            "1, 1, 1, 1, 0     1835\n",
            "1, 1, 1, 1, 0     1840\n",
            "1, 1, 0, 1, 0     1845\n",
            "1, 0, 1, 1, 0     1847\n",
            "1, 1, 1, 1, 0     1848\n",
            "1, 1, 1, 1, 0     1850\n",
            "1, 0, 1, 0, 1     1853\n",
            "1, 1, 1, 1, 0     1859\n",
            "1, 0, 1, 0, 0     1860\n",
            "1, 1, 1, 1, 0     1867\n",
            "1, 1, 1, 1, 0     1870\n",
            "0, 0, 1, 0, 1     1871\n",
            "0, 1, 0, 0, 1     1874\n",
            "1, 0, 1, 1, 0     1877\n",
            "1, 1, 1, 1, 0     1883\n",
            "1, 0, 0, 1, 0     1884\n",
            "1, 1, 1, 1, 0     1891\n",
            "0, 0, 1, 1, 1     1892\n",
            "1, 1, 1, 1, 0     1893\n",
            "1, 0, 0, 1, 1     1903\n",
            "1, 0, 1, 0, 0     1905\n",
            "0, 0, 1, 0, 1     1915\n",
            "1, 0, 1, 1, 0     1931\n",
            "1, 1, 1, 1, 0     1934\n",
            "1, 1, 1, 0, 0     1935\n",
            "1, 1, 1, 1, 0     1942\n",
            "0, 1, 1, 0, 1     1944\n",
            "0, 0, 1, 1, 1     1952\n",
            "1, 1, 1, 1, 0     1965\n",
            "1, 0, 0, 0, 1     1966\n",
            "1, 1, 0, 1, 0     1970\n",
            "1, 1, 1, 1, 0     1975\n",
            "1, 1, 1, 1, 0     1976\n",
            "1, 0, 1, 1, 0     1979\n",
            "0, 0, 1, 0, 1     1991\n",
            "1, 1, 1, 1, 0     1992\n",
            "0, 0, 1, 0, 1     1993\n",
            "1, 0, 0, 1, 0     1997\n",
            "0, 0, 1, 1, 1     2003\n",
            "1, 0, 1, 1, 0     2008\n",
            "0, 0, 1, 0, 1     2009\n",
            "1, 1, 1, 1, 0     2015\n",
            "1, 1, 1, 1, 0     2022\n",
            "1, 0, 1, 1, 0     2026\n",
            "1, 0, 1, 0, 0     2028\n",
            "0, 0, 1, 0, 1     2031\n",
            "0, 0, 1, 1, 0     2036\n",
            "1, 1, 1, 1, 0     2041\n",
            "1, 1, 1, 1, 0     2046\n",
            "1, 0, 1, 0, 1     2061\n",
            "1, 1, 1, 1, 0     2062\n",
            "0, 0, 0, 0, 1     2063\n",
            "1, 0, 1, 1, 0     2076\n",
            "1, 1, 1, 1, 0     2077\n",
            "1, 1, 1, 1, 0     2078\n",
            "1, 1, 1, 1, 0     2082\n",
            "1, 1, 1, 1, 0     2091\n",
            "1, 1, 1, 1, 0     2102\n",
            "1, 1, 1, 1, 0     2105\n",
            "1, 1, 1, 0, 0     2111\n",
            "1, 0, 1, 0, 0     2115\n",
            "0, 0, 0, 0, 1     2120\n",
            "1, 1, 1, 1, 0     2123\n",
            "1, 1, 1, 1, 0     2125\n",
            "1, 0, 0, 1, 0     2131\n",
            "1, 0, 1, 1, 0     2133\n",
            "1, 1, 1, 1, 0     2138\n",
            "1, 1, 1, 1, 0     2140\n",
            "1, 0, 0, 0, 1     2141\n",
            "1, 0, 1, 0, 1     2144\n",
            "1, 1, 1, 1, 0     2152\n",
            "1, 0, 1, 1, 0     2154\n",
            "1, 1, 1, 1, 0     2156\n",
            "1, 0, 0, 0, 1     2161\n",
            "1, 0, 1, 0, 0     2169\n",
            "1, 1, 1, 1, 0     2171\n",
            "1, 0, 1, 0, 1     2178\n",
            "1, 1, 1, 1, 0     2181\n",
            "0, 0, 1, 0, 1     2186\n",
            "0, 0, 0, 0, 1     2189\n",
            "1, 1, 0, 1, 0     2192\n",
            "1, 1, 1, 1, 0     2198\n",
            "0, 1, 1, 0, 0     2201\n",
            "1, 0, 1, 1, 0     2203\n",
            "1, 0, 1, 1, 0     2204\n",
            "1, 1, 1, 1, 0     2206\n",
            "1, 1, 1, 1, 0     2209\n",
            "1, 1, 1, 0, 0     2211\n",
            "1, 1, 1, 1, 0     2212\n",
            "1, 1, 1, 1, 0     2213\n",
            "1, 1, 1, 0, 0     2217\n",
            "0, 1, 1, 1, 0     2218\n",
            "1, 0, 1, 1, 0     2225\n",
            "1, 1, 1, 1, 0     2228\n",
            "1, 0, 1, 0, 1     2230\n",
            "1, 1, 1, 1, 0     2237\n",
            "0, 1, 0, 0, 1     2238\n",
            "1, 1, 0, 1, 0     2244\n",
            "0, 0, 1, 0, 1     2251\n",
            "1, 1, 1, 1, 0     2258\n",
            "1, 0, 0, 0, 1     2260\n",
            "0, 0, 0, 0, 1     2264\n",
            "1, 1, 1, 1, 0     2267\n",
            "1, 1, 1, 1, 0     2269\n",
            "1, 1, 1, 1, 0     2273\n",
            "1, 1, 1, 1, 0     2274\n",
            "1, 1, 1, 0, 0     2288\n",
            "0, 0, 0, 1, 1     2293\n",
            "1, 1, 1, 1, 0     2295\n",
            "1, 1, 1, 1, 0     2298\n",
            "1, 1, 1, 1, 0     2301\n",
            "1, 1, 1, 1, 0     2303\n",
            "1, 1, 1, 1, 0     2304\n",
            "1, 1, 1, 1, 0     2305\n",
            "1, 1, 1, 1, 0     2313\n",
            "1, 1, 1, 1, 0     2314\n",
            "1, 1, 1, 1, 0     2324\n",
            "1, 1, 1, 1, 0     2327\n",
            "0, 0, 0, 0, 1     2332\n",
            "1, 1, 1, 1, 0     2342\n",
            "1, 1, 1, 1, 0     2343\n",
            "1, 1, 1, 1, 0     2344\n",
            "1, 1, 1, 1, 0     2347\n",
            "1, 1, 1, 1, 0     2350\n",
            "1, 0, 1, 1, 0     2353\n",
            "1, 0, 1, 1, 0     2363\n",
            "1, 1, 1, 1, 0     2365\n",
            "0, 0, 1, 0, 1     2368\n",
            "1, 1, 1, 1, 0     2372\n",
            "1, 1, 1, 1, 0     2384\n",
            "0, 0, 1, 0, 1     2386\n",
            "1, 1, 1, 1, 0     2390\n",
            "1, 1, 0, 1, 0     2394\n",
            "1, 1, 1, 1, 0     2396\n",
            "1, 1, 1, 1, 0     2400\n",
            "1, 0, 1, 1, 0     2415\n",
            "1, 1, 1, 1, 0     2416\n",
            "1, 0, 0, 1, 0     2419\n",
            "1, 1, 0, 0, 0     2421\n",
            "1, 0, 1, 1, 0     2424\n",
            "1, 1, 1, 1, 0     2428\n",
            "1, 1, 1, 1, 0     2447\n",
            "1, 1, 1, 1, 0     2454\n",
            "1, 1, 1, 1, 0     2457\n",
            "1, 1, 1, 1, 0     2459\n",
            "1, 1, 1, 0, 0     2464\n",
            "0, 1, 1, 1, 0     2471\n",
            "1, 1, 0, 1, 0     2475\n",
            "0, 1, 0, 0, 1     2480\n",
            "1, 0, 1, 1, 0     2484\n",
            "1, 1, 1, 1, 0     2492\n",
            "1, 1, 1, 1, 0     2495\n",
            "1, 1, 1, 1, 0     2500\n",
            "1, 1, 1, 0, 0     2502\n",
            "1, 1, 1, 1, 0     2507\n",
            "1, 1, 0, 0, 1     2508\n",
            "0, 0, 0, 0, 1     2513\n",
            "1, 0, 1, 1, 0     2514\n",
            "1, 1, 1, 1, 0     2520\n",
            "1, 1, 1, 1, 0     2523\n",
            "1, 0, 0, 1, 1     2527\n",
            "1, 1, 1, 0, 0     2529\n",
            "1, 0, 1, 1, 0     2542\n",
            "1, 1, 1, 1, 0     2547\n",
            "1, 0, 1, 1, 0     2549\n",
            "0, 1, 0, 0, 1     2550\n",
            "1, 0, 0, 1, 1     2552\n",
            "0, 0, 0, 1, 1     2554\n",
            "1, 1, 1, 1, 0     2556\n",
            "0, 0, 1, 0, 1     2558\n",
            "1, 0, 1, 0, 1     2559\n",
            "1, 1, 1, 1, 0     2561\n",
            "1, 1, 1, 1, 0     2564\n",
            "1, 1, 1, 1, 0     2566\n",
            "1, 1, 1, 1, 0     2568\n",
            "1, 1, 1, 1, 0     2572\n",
            "1, 1, 0, 1, 0     2576\n",
            "1, 1, 1, 1, 0     2581\n",
            "1, 1, 1, 1, 0     2588\n",
            "0, 0, 0, 0, 1     2592\n",
            "1, 1, 1, 1, 0     2596\n",
            "0, 1, 1, 1, 0     2601\n",
            "1, 1, 1, 1, 0     2603\n",
            "1, 0, 1, 0, 1     2611\n",
            "1, 0, 0, 1, 1     2618\n",
            "1, 1, 0, 1, 0     2619\n",
            "1, 0, 0, 0, 1     2625\n",
            "1, 1, 1, 1, 0     2626\n",
            "1, 1, 1, 1, 0     2630\n",
            "1, 0, 1, 1, 0     2633\n",
            "1, 1, 1, 1, 0     2643\n",
            "1, 1, 1, 1, 0     2646\n",
            "1, 1, 1, 1, 0     2652\n",
            "0, 1, 0, 1, 0     2653\n",
            "1, 0, 0, 0, 1     2657\n",
            "0, 0, 0, 0, 1     2659\n",
            "1, 0, 1, 0, 0     2661\n",
            "1, 0, 1, 1, 0     2663\n",
            "0, 1, 1, 0, 1     2666\n",
            "1, 1, 1, 1, 0     2673\n",
            "1, 1, 1, 1, 0     2678\n",
            "1, 1, 1, 1, 0     2679\n",
            "0, 0, 0, 0, 1     2682\n",
            "1, 0, 1, 1, 0     2695\n",
            "0, 0, 1, 0, 1     2700\n",
            "1, 1, 1, 1, 0     2702\n",
            "0, 0, 1, 0, 1     2706\n",
            "1, 1, 1, 1, 0     2716\n",
            "1, 1, 1, 1, 0     2724\n",
            "1, 1, 1, 1, 0     2731\n",
            "1, 1, 0, 0, 1     2733\n",
            "1, 0, 1, 1, 0     2739\n",
            "0, 1, 1, 0, 0     2743\n",
            "1, 1, 1, 1, 0     2746\n",
            "1, 0, 1, 1, 0     2749\n",
            "0, 1, 1, 1, 0     2752\n",
            "1, 1, 1, 1, 0     2753\n",
            "1, 1, 1, 1, 0     2760\n",
            "1, 1, 1, 1, 0     2764\n",
            "1, 1, 1, 0, 0     2765\n",
            "1, 1, 1, 0, 0     2767\n",
            "1, 1, 1, 1, 0     2776\n",
            "0, 0, 0, 0, 1     2777\n",
            "1, 1, 0, 0, 1     2781\n",
            "0, 0, 1, 1, 0     2782\n",
            "1, 1, 1, 1, 0     2784\n",
            "1, 0, 0, 1, 0     2785\n",
            "1, 0, 1, 1, 0     2791\n",
            "1, 1, 1, 1, 0     2798\n",
            "1, 0, 0, 0, 1     2801\n",
            "0, 0, 0, 0, 1     2819\n",
            "1, 1, 1, 1, 0     2824\n",
            "1, 1, 1, 1, 0     2825\n",
            "1, 1, 1, 1, 0     2831\n",
            "1, 1, 1, 0, 0     2832\n",
            "1, 1, 1, 1, 0     2833\n",
            "1, 1, 1, 1, 0     2839\n",
            "1, 1, 1, 1, 0     2844\n",
            "1, 1, 1, 1, 0     2854\n",
            "1, 1, 1, 0, 0     2858\n",
            "1, 1, 1, 1, 0     2864\n",
            "1, 1, 1, 1, 0     2865\n",
            "0, 0, 0, 0, 1     2871\n",
            "0, 0, 1, 0, 1     2872\n",
            "1, 1, 1, 1, 0     2874\n",
            "1, 1, 1, 1, 0     2882\n",
            "1, 1, 1, 1, 0     2893\n",
            "1, 0, 1, 1, 0     2895\n",
            "1, 1, 1, 1, 0     2901\n",
            "1, 0, 0, 0, 1     2902\n",
            "1, 1, 1, 1, 0     2910\n",
            "1, 1, 1, 1, 0     2911\n",
            "0, 0, 1, 0, 1     2919\n",
            "1, 0, 1, 1, 0     2920\n",
            "0, 0, 0, 0, 1     2924\n",
            "1, 1, 1, 1, 0     2933\n",
            "1, 1, 1, 0, 0     2935\n",
            "1, 1, 1, 1, 0     2938\n",
            "1, 1, 1, 1, 0     2941\n",
            "1, 1, 1, 1, 0     2944\n",
            "1, 0, 0, 1, 1     2950\n",
            "1, 0, 0, 1, 0     2957\n",
            "1, 1, 1, 1, 0     2960\n",
            "1, 0, 1, 1, 0     2975\n",
            "1, 1, 1, 1, 0     2981\n",
            "1, 0, 1, 1, 0     2990\n",
            "0, 0, 1, 0, 1     2992\n",
            "1, 1, 1, 1, 0     2993\n",
            "1, 1, 1, 0, 0     2997\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}